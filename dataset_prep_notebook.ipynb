{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial_id",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from transformers import AutoTokenizer\n",
    "from datasets import Dataset, DatasetDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "base_model_id = \"mistralai/Mistral-7B-v0.1\"\n",
    "model_max_length = 512\n",
    "dataset_name = 'alpaca_code'\n",
    "output_dir = \"./datasets/\" + dataset_name\n",
    "eos_token = '</s>'"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "89d14aaade474cfd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset = pd.read_json(f'./data/{dataset_name}.json')"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b89e884daac87710"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def gen_dataset_splits(ds, perc: [], verbose:False):\n",
    "    ds_len = len(ds)\n",
    "    train_len = int(ds_len * perc[0])\n",
    "    eval_len = int(ds_len * perc[1])\n",
    "    test_len = ds_len - train_len - eval_len\n",
    "    if verbose:\n",
    "        print(f\"train size: {train_len}, validation size:{eval_len}, test size:{test_len} - total size: {ds_len}\")\n",
    "    splits = np.concatenate([\n",
    "        np.zeros(train_len),\n",
    "        np.ones(eval_len),\n",
    "        np.full(test_len, 2)\n",
    "    ])\n",
    "    np.random.shuffle(splits)\n",
    "    return splits"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "e18ff0bba1292ec3"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset['split'] = gen_dataset_splits(dataset, [.9, .01], verbose=True)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "7afea6e6d7a2bdb"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def alpaca_prompt(row):\n",
    "    return (\"Below is an instruction that describes a task. \"\n",
    "            \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "            \"### Instruction:\\n{instruction}\\n\\n### Response:\\n\").format_map(row)\n",
    "\n",
    "\n",
    "def alpaca_prompt_input(row):\n",
    "    return (\"Below is an instruction that describes a task, paired with an input that provides further context. \"\n",
    "            \"Write a response that appropriately completes the request.\\n\\n\"\n",
    "            \"### Instruction:\\n{instruction}\\n\\n### Input:\\n{input}\\n\\n### Response:\\n\").format_map(row)\n",
    "\n",
    "\n",
    "def gen_prompt(row):\n",
    "    return (alpaca_prompt(row) if row['input'] == \"\" else alpaca_prompt_input(row)) + row['output'] + eos_token"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b218f4d19049bd80"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "dataset['prompt'] = dataset.apply(lambda x: gen_prompt(x), axis=1)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "bb1879f50437f85b"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(base_model_id, model_max_length=model_max_length)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c17ef4af85e5cf0e"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# split dataset\n",
    "def get_split(ds, split_id=0):\n",
    "    res = ds[ds['split'] == split_id]\n",
    "    res = res.drop('split', axis=1)\n",
    "    return res\n",
    "\n",
    "\n",
    "train_dataset = get_split(dataset)\n",
    "eval_dataset = get_split(dataset, 1)\n",
    "test_dataset = get_split(dataset, 2)\n",
    "\n",
    "# extract prompts\n",
    "train_prompts = train_dataset['prompt'].to_list()\n",
    "eval_prompts = eval_dataset['prompt'].to_list()\n",
    "test_prompts = test_dataset['prompt'].to_list()"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "70febee7a8474158"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# tokenize\n",
    "def tokenize(prompts, tokenizer):\n",
    "    return tokenizer(prompts, truncation=True)['input_ids']\n",
    "\n",
    "\n",
    "tokenized_train_dataset = tokenize(train_prompts, tokenizer)\n",
    "tokenized_eval_dataset = tokenize(eval_prompts, tokenizer)\n",
    "tokenized_test_dataset = tokenize(test_prompts, tokenizer)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "38a7984e29f01884"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# packing\n",
    "def pack(tokens, max_model_size):\n",
    "    packed_tokens = []\n",
    "    i = 0\n",
    "    pack = []\n",
    "    while i < len(tokens):\n",
    "        cur_len = len(pack)\n",
    "        if cur_len + len(tokens[i]) <= max_model_size:\n",
    "            pack.extend(tokens[i])\n",
    "        else:\n",
    "            packed_tokens.append(pack)\n",
    "            pack = tokens[i]\n",
    "        i += 1\n",
    "    if len(pack) > 0:\n",
    "        packed_tokens.append(pack)\n",
    "    return packed_tokens\n",
    "\n",
    "packed_train_data = pack(tokenized_train_dataset, model_max_length)\n",
    "packed_eval_data = pack(tokenized_eval_dataset, model_max_length)\n",
    "packed_test_data = pack(tokenized_test_dataset, model_max_length)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "b7c7540a284268b5"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# padding\n",
    "def pad(tokens, max_model_size, pad_token_id=2):\n",
    "    padded_tokens = []\n",
    "    for i in tokens:\n",
    "        cur_len = len(i)\n",
    "        if cur_len >= max_model_size:\n",
    "            padded_tokens.append(i)\n",
    "            continue\n",
    "        needed_padding = max_model_size - cur_len\n",
    "        pad = np.full(needed_padding, pad_token_id)\n",
    "        i.extend(pad)\n",
    "        padded_tokens.append(i)\n",
    "    return padded_tokens\n",
    "\n",
    "packed_padded_train_data = pad(packed_train_data, model_max_length, tokenizer.eos_token_id)\n",
    "packed_padded_eval_data = pad(packed_eval_data, model_max_length, tokenizer.eos_token_id)\n",
    "packed_padded_test_data = pad(packed_test_data, model_max_length, tokenizer.eos_token_id)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "41101f87fe3fe5e1"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# save to disk\n",
    "tds = Dataset.from_dict({'input_ids': packed_padded_train_data, 'labels': packed_padded_train_data})\n",
    "eds = Dataset.from_dict({'input_ids': packed_padded_eval_data, 'labels': packed_padded_eval_data})\n",
    "teds = Dataset.from_dict({'input_ids': packed_padded_test_data, 'labels': packed_padded_test_data})\n",
    "\n",
    "ds = DatasetDict({\n",
    "    'train': tds,\n",
    "    \"eval\": eds,\n",
    "    'test': teds\n",
    "})\n",
    "\n",
    "ds.save_to_disk(output_dir)"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "8d02598b9f0fd9b8"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "71ab7a1b8b570bb2"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
